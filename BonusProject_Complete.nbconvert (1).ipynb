{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# preparing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/share/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:1: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "data=pd.read_csv(\"ratings.dat\",sep=\"::\",header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1193</td>\n",
       "      <td>5</td>\n",
       "      <td>978300760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>661</td>\n",
       "      <td>3</td>\n",
       "      <td>978302109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>914</td>\n",
       "      <td>3</td>\n",
       "      <td>978301968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3408</td>\n",
       "      <td>4</td>\n",
       "      <td>978300275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>2355</td>\n",
       "      <td>5</td>\n",
       "      <td>978824291</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   0     1  2          3\n",
       "0  1  1193  5  978300760\n",
       "1  1   661  3  978302109\n",
       "2  1   914  3  978301968\n",
       "3  1  3408  4  978300275\n",
       "4  1  2355  5  978824291"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.rename(columns={0:'UserID',1:'MovieID',2:'Rating',3:'Timestamp'},inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>UserID</th>\n",
       "      <th>MovieID</th>\n",
       "      <th>Rating</th>\n",
       "      <th>Timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1193</td>\n",
       "      <td>5</td>\n",
       "      <td>978300760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>661</td>\n",
       "      <td>3</td>\n",
       "      <td>978302109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>914</td>\n",
       "      <td>3</td>\n",
       "      <td>978301968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3408</td>\n",
       "      <td>4</td>\n",
       "      <td>978300275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>2355</td>\n",
       "      <td>5</td>\n",
       "      <td>978824291</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   UserID  MovieID  Rating  Timestamp\n",
       "0       1     1193       5  978300760\n",
       "1       1      661       3  978302109\n",
       "2       1      914       3  978301968\n",
       "3       1     3408       4  978300275\n",
       "4       1     2355       5  978824291"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=data[['UserID','MovieID','Rating']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## dataset of 1 million M dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000209"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## clean the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LabelEncoder()"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Users_label=LabelEncoder()\n",
    "Users_label.fit(data['UserID'])\n",
    "Movie_label=LabelEncoder()\n",
    "Movie_label.fit(data['MovieID'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Users_label.transform([1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"Users\"]=Users_label.transform(data.UserID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"Movies\"]=Movie_label.transform(data.MovieID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>UserID</th>\n",
       "      <th>MovieID</th>\n",
       "      <th>Rating</th>\n",
       "      <th>Users</th>\n",
       "      <th>Movies</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1193</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>1104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>661</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>914</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>853</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3408</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>3177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>2355</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>2162</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   UserID  MovieID  Rating  Users  Movies\n",
       "0       1     1193       5      0    1104\n",
       "1       1      661       3      0     639\n",
       "2       1      914       3      0     853\n",
       "3       1     3408       4      0    3177\n",
       "4       1     2355       5      0    2162"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we only make use of the used_data1 to do our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "used_data=data.pivot(index=\"Users\",columns=\"Movies\",values='Rating')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "try_data=np.ones_like(used_data.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## split the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### training, validation, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train,test=train_test_split(data[['Rating','Users','Movies']],test_size=0.2,random_state=999)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### change them into numpy.array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train,test=train.values,test.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('float64')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "try_data.dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. u=1,000,000\n",
    "2. r=1,2,3,4,5\n",
    "\n",
    "Goal: predict r_(u,i) for those unobserved objects\n",
    "\n",
    "3. valuation: MSE\n",
    "\n",
    "4. you need to do a crossvalidation in order to find the best space\n",
    "\n",
    "5. we will learn b_u, b_i, p_u, p_i 4 kinds of factors, and P_u U F, P_i I F in total. F-> factors of that movies.\n",
    "\n",
    "6. for the loss function. \n",
    "\n",
    "7. training: training with stochastic gradient descent. with a batch_size. \n",
    "\n",
    "8. pay attention to the parameter initialization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialization(data:np.ndarray,k:int):\n",
    "    '''\n",
    "    data: the frame of the whole dataset: a frame that has n rows of users and m cols of movies\n",
    "          the data in this frame is not important\n",
    "    '''\n",
    "    users=data.shape[0]\n",
    "    movies=data.shape[1]\n",
    "    b_u=np.random.normal(0,1e-4,users)\n",
    "    b_i=np.random.normal(0,1e-4,movies)\n",
    "    \n",
    "    for_pq=1/(max(1,np.sqrt(k)))\n",
    "    pu=np.random.normal(0,for_pq,[users,k])\n",
    "    pi=np.random.normal(0,for_pq,[movies,k])\n",
    "    \n",
    "    \n",
    "    u=np.nanmean(data)\n",
    "    return u,b_u,b_i,pu,pi\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(matrix_,u,b_u,b_i,pu,pi,lambda_):\n",
    "    '''\n",
    "    matrix_: the true matrix of that ratings\n",
    "    u: the mean of all data\n",
    "    b_u, b_i  user bias and movie bias\n",
    "    pu,pi user and movie embeddings\n",
    "    '''\n",
    "    matrix_telda=u+b_u[:,np.newaxis]+b_i[np.newaxis,:]+np.dot(pu,pi.T)\n",
    "    matrix_telda=(matrix_-matrix_telda)**2+ lambda_* (np.sum(pu**2,1)[:,np.newaxis]+np.sum(pi**2,1)[np.newaxis,:])/2\n",
    "#     N=(np.isnan(matrix_telda)==False).sum()\n",
    "    \n",
    "#     return np.nansum(matrix_telda)\n",
    "    return np.nanmean(matrix_telda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  using the index to extract the matrix\n",
    "def loss_use(matrix_,u,b_u,b_i,pu,pi,user_index,movie_index,lambda_):\n",
    "    '''\n",
    "    matrix_: the true matrix of that ratings\n",
    "    u: the mean of all data\n",
    "    b_u, b_i  user bias and movie bias\n",
    "    pu,pi user and movie embeddings\n",
    "    '''\n",
    "    row=user_index[:,np.newaxis]\n",
    "    col=movie_index[np.newaxis,:]\n",
    "    return loss(matrix_[row,col],u,b_u[user_index],b_i[movie_index],pu[user_index],pi[movie_index],lambda_ )    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count(matrix_,user_index,movie_index):\n",
    "    '''\n",
    "    counting how many non-nan values in our batch matrix selected for the training\n",
    "    '''\n",
    "    row=user_index[:,np.newaxis]\n",
    "    col=movie_index[np.newaxis,:]\n",
    "    N=np.sum(np.isnan(matrix_[row,col])==False)\n",
    "    return N\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# matrix,u,b_u,b_i,pu,pi,user_index,movie_index,lambda_=try_data,u,b_u,b_i,pu,pi,a,b,10\n",
    "\n",
    "def gradient(matrix,u,b_u,b_i,pu,pi,user_index,movie_index,lambda_):\n",
    "    '''\n",
    "    output the gradient for selected users and movies\n",
    "    '''\n",
    "    \n",
    "    row=user_index[:,np.newaxis]\n",
    "    col=movie_index[np.newaxis,:]\n",
    "\n",
    "    rate_telda=u+b_u[user_index][:,np.newaxis]+b_i[movie_index][np.newaxis,:]+np.dot(pu[user_index],pi[movie_index].T)\n",
    "\n",
    "    dif=matrix[row,col]-rate_telda\n",
    "\n",
    "    b_u_gra=-np.nansum(dif,1)\n",
    "    b_i_gra=-np.nansum(dif,0)\n",
    "\n",
    "    # np.nanprod()\n",
    "\n",
    "    pu_cal=dif[:,np.newaxis,:]*pi[movie_index].T[np.newaxis,:,:]\n",
    "    pi_cal=dif.T[:,np.newaxis,:]*pu[user_index].T[np.newaxis,:,:]\n",
    "\n",
    "    pu_cal=-pu_cal+lambda_*pu[user_index][:,:,np.newaxis]\n",
    "    pi_cal=-pi_cal+lambda_*pi[movie_index][:,:,np.newaxis]\n",
    "\n",
    "    pu_gra=np.nansum(pu_cal,2)\n",
    "    pi_gra=np.nansum(pi_cal,2)\n",
    "    # pu_gra=-np.dot(dif,pi[movie_index].T)+lambda_*pu[user_index]\n",
    "    # pi_gra=-np.dot(dif.T,pu[user_index])+lambda_*pi[movie_index]\n",
    "\n",
    "    #  due to NaN\n",
    "\n",
    "    return b_u_gra,b_i_gra,pu_gra,pi_gra\n",
    "\n",
    "# gradient returns: gradients of bu, bi, pu, pi, and their position ( row, col)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## generate a training functions with epochs\n",
    "\n",
    "parameters: F etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Batch_generate(data,B):\n",
    "    '''\n",
    "    output a generation for the data\n",
    "    data: the data for training or testing\n",
    "    B: batch size\n",
    "    '''\n",
    "    if len(data)%B==0:\n",
    "        upper=len(data)//B\n",
    "        np.random.shuffle(data)\n",
    "        for i in range(upper):\n",
    "            yield(data[i*B:(i+1)*B])\n",
    "    else:\n",
    "        upper=len(data)//B\n",
    "        np.random.shuffle(data)\n",
    "        for i in range(upper+1):\n",
    "            yield(data[i*B:(i+1)*B])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# training \n",
    "<p> our goal is to select the best F </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def updating(para,gradient,learning_rate,row):\n",
    "    '''\n",
    "    used for updating the parameters with the gradient\n",
    "    para: matrix: the parameter\n",
    "    gradient: gradient\n",
    "    learning_rate\n",
    "    row: np.array 1D the position of the parameter\n",
    "    '''\n",
    "    para[row]-=learning_rate*gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### check the gradient\n",
    "\n",
    "before training on the whole dataset,\n",
    "I will select a batch to see whether the gradient and optimization function will help decrease the loss and reach the optimal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6040, 3706)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "try_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "u,b_u,b_i,pu,pi=initialization(try_data,8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# b_u_gra,b_i_gra,pu_gra,pi_gra=gradient(try_data,u,b_u,b_i,pu,pi,user_index,movie_index,10)\n",
    "gen=Batch_generate(train,100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate=0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_=next(gen)\n",
    "user_index,movie_index=all_[:,1],all_[:,2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_list_temp=[]\n",
    "for i in range(10):\n",
    "    b_u_gra,b_i_gra,pu_gra,pi_gra=gradient(try_data,u,b_u,b_i,pu,pi,user_index,movie_index,10)\n",
    "    updating(b_u,b_u_gra,learning_rate,user_index)\n",
    "    updating(b_i,b_i_gra,learning_rate,movie_index)\n",
    "    updating(pu,pu_gra,learning_rate,user_index)\n",
    "    updating(pi,pi_gra,learning_rate,movie_index)\n",
    "    loss_list_temp.append(loss_use(try_data,u,b_u,b_i,pu,pi,user_index,movie_index,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f40588d2f50>]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(loss_list_temp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Should be correct"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## write a class for training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class learning:\n",
    "    def __init__(self,whole_data:np.array,k,lambda_,learning_rate,B=100,epoch=20,show_epoch=5):\n",
    "        '''\n",
    "        initilization \n",
    "        \n",
    "        whole_data: the frame of users: only a frame initializing the model's parameter\n",
    "        k: number of latent factors for users and movies\n",
    "        lambda_: penalty for training\n",
    "        B: batch size\n",
    "        epoch: number of epoch you want to train the model\n",
    "        show_epoch: frequency you need to show the loss during training\n",
    "        \n",
    "        in this class, we only output (show) the loss(MSE) without panalty, which is what the question required\n",
    "        but we will still compute the gradient with panalty, which is required\n",
    "        \n",
    "        '''\n",
    "        self.data=whole_data*np.NaN\n",
    "#         clear the frame work\n",
    "\n",
    "        self.lambda_form=lambda_\n",
    "        self.learning_rate=learning_rate\n",
    "        self.B=B\n",
    "        self.epoch=epoch\n",
    "        self.show_epoch=show_epoch\n",
    "        self.k=k\n",
    "        \n",
    "        print(\"generating model with {} parameters\".format(self.k))\n",
    "\n",
    "    \n",
    "    def clear_data(self,instance):\n",
    "#         self.data=self.data*np.NaN\n",
    "#         clear the frame after each use of input_data\n",
    "        for i in instance:\n",
    "            self.data[i[1],i[2]]=np.NaN\n",
    "    \n",
    "    def input_data(self,instance):\n",
    "#         for each batch, input the data into the frame\n",
    "        for i in instance:\n",
    "            self.data[i[1],i[2]]=i[0]\n",
    "    \n",
    "    def loss_(self,user_index,movie_index,train=True):\n",
    "        '''\n",
    "        only calculating the loss. lambda is not used here since the true MSE will not be taken into account\n",
    "        '''\n",
    "        return loss_use(self.data,self.u,self.b_u,self.b_i,self.pu,self.pi,user_index,movie_index,0)\n",
    "#         if not training here, just directly return the loss error with out penalty \n",
    "        \n",
    "    def gradient(self,user_index,movie_index):\n",
    "        self.b_u_gra,self.b_i_gra,self.pu_gra,self.pi_gra=gradient(self.data,self.u,self.b_u,self.b_i,self.pu,self.pi,\\\n",
    "                                                                  user_index,movie_index,self.lambda_)\n",
    "    \n",
    "    def update(self,user_index,movie_index):\n",
    "        self.gradient(user_index,movie_index)\n",
    "        updating(self.b_u,self.b_u_gra,self.learning_rate,user_index)\n",
    "        updating(self.b_i,self.b_i_gra,self.learning_rate,movie_index)\n",
    "        updating(self.pu,self.pu_gra,self.learning_rate,user_index)\n",
    "        updating(self.pi,self.pi_gra,self.learning_rate,movie_index)\n",
    "        \n",
    "        \n",
    "    def generating_batch(self,train_data):\n",
    "        '''\n",
    "        generating the batch generator\n",
    "        train_data: np.array 2D n*m; rows: n ratings, cols: rating, user position, movie position\n",
    "        \n",
    "        '''\n",
    "        self.batch_generator=Batch_generate(train_data,self.B)\n",
    "        if len(train_data)%self.B:\n",
    "            self.loops=len(train_data)//self.B+1\n",
    "        else:\n",
    "            self.loops=len(train_data)//self.B\n",
    "        self.lambda_=self.lambda_form/self.loops\n",
    "        \n",
    "# -------------------Notation---------------------\n",
    "# here the lambda_ which is actually place into the batch, should devided by self.loops, in order that follow the same \n",
    "# loss function with panalty\n",
    "# ------------------------------------------------\n",
    "            \n",
    "\n",
    "    def fit(self,train_data):\n",
    "        '''\n",
    "        fit the model with training set\n",
    "        '''\n",
    "#       input the data to generating the mean: u and other parameters.\n",
    "        self.input_data(train_data)       \n",
    "        self.u,self.b_u,self.b_i,self.pu,self.pi=initialization(self.data,self.k)    \n",
    "        self.clear_data(train_data)\n",
    "#         clear our data frame\n",
    "#         first lets calculate the original error\n",
    "        print(\"training start: initial total loss: {}\".format(self.all_loss(train_data)))        \n",
    "        print(\"-----------------------------training begin--------------------------------\")\n",
    "        for i_ in range(self.epoch):\n",
    "            self.generating_batch(train_data)\n",
    "            total_loss=0\n",
    "            for i in range(self.loops):\n",
    "                all_=next(self.batch_generator)\n",
    "                self.input_data(all_)\n",
    "#                 input the data with the instruction from batch B\n",
    "                user_index,movie_index=all_[:,1],all_[:,2]\n",
    "#               get the position of users and movies\n",
    "#               and get rid of the duplicates\n",
    "                user_index,movie_index=np.unique(user_index),np.unique(movie_index)\n",
    "#               update the parameters.\n",
    "                self.update(user_index,movie_index)\n",
    "                self.clear_data(all_)\n",
    "            if i_%self.show_epoch==0:\n",
    "                print(\"training epoch {} : total loss: {}\".format(i_+1,self.all_loss(train_data)))\n",
    "#         self.clear_data()\n",
    "    \n",
    "    \n",
    "    \n",
    "    def all_loss(self,test_data):\n",
    "        '''\n",
    "        calculate the total loss\n",
    "        \n",
    "        test_data: the data you need to calculate the loss\n",
    "        np.array 2D n*m; rows: n ratings, cols: rating, user position, movie position\n",
    "        \n",
    "        '''\n",
    "#         self.input_data(test_data)\n",
    "        self.generating_batch(test_data)\n",
    "#     for easily calculation, use small batch size for the total loss\n",
    "        total_loss=0\n",
    "        for i in range(self.loops):\n",
    "            all_=next(self.batch_generator)\n",
    "            self.input_data(all_)\n",
    "            user_index,movie_index=all_[:,1],all_[:,2]\n",
    "#             print(\"all_:\",len(all_))\n",
    "            user_index,movie_index=np.unique(user_index),np.unique(movie_index)\n",
    "#           some of the batch will consist of same user_id or same movie_id, you only need to use the unique one.         \n",
    "            loss_here=self.loss_(user_index,movie_index,train=False)\n",
    "            total_loss+=(loss_here/self.loops)\n",
    "            self.clear_data(all_)\n",
    "        return total_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Popularity model: F=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generating model with 0 parameters\n"
     ]
    }
   ],
   "source": [
    "first=learning(try_data,k=0,lambda_=1,learning_rate=0.001,epoch=20,show_epoch=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_here,validate_here=train_test_split(train,test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training start: initial total loss: 1.2491530478948587\n",
      "-----------------------------training begin--------------------------------\n",
      "training epoch 1 : total loss: 1.046180567339768\n",
      "training epoch 3 : total loss: 0.9281977930330627\n",
      "training epoch 5 : total loss: 0.8852602240656732\n",
      "training epoch 7 : total loss: 0.8628469477021498\n",
      "training epoch 9 : total loss: 0.8491476367810581\n",
      "training epoch 11 : total loss: 0.8398767533893727\n",
      "training epoch 13 : total loss: 0.8331330415453427\n",
      "training epoch 15 : total loss: 0.8281290061887\n",
      "training epoch 17 : total loss: 0.8242842793411012\n",
      "training epoch 19 : total loss: 0.8212415516487765\n"
     ]
    }
   ],
   "source": [
    "first.fit(train_here)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8355861207360319"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first.all_loss(validate_here)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the loss for F=0 with lambda_=1,learning_rate=0.001,epoch=20 in the validation set is 0.8355"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## choose good parameters\n",
    "\n",
    "in thie section, only test the parameters of penalty and learning rate\n",
    "\n",
    "we will base on the F=6 to choose the hyper-parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we do a gridsearch here to choose the 'look good' parameters for our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.shuffle(train)\n",
    "train_temp_data=train_here[:5000]\n",
    "# we will use the 10000 ratings to choose good parameters, and 5000 as validation to choose the good hyper-parameters.\n",
    "validation_temp_data=validate_here[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambda_list=[0.01,0.1,1,100]\n",
    "learning_rate_list=[0.005,0.01]\n",
    "# before thie, 0.1 is a bad idea since usually pretty big\n",
    "batch_size=[400,1000]\n",
    "epochs=[20,40]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generating model with 6 parameters\n",
      "training start: initial total loss: 1.4274225111493055\n",
      "-----------------------------training begin--------------------------------\n",
      "training epoch 1 : total loss: 1.3565211681702816\n",
      "training epoch 6 : total loss: 1.0722609573435238\n",
      "training epoch 11 : total loss: 0.8655769431430679\n",
      "training epoch 16 : total loss: 0.7189215684890797\n",
      "generating model with 6 parameters\n",
      "training start: initial total loss: 1.4421992917575877\n",
      "-----------------------------training begin--------------------------------\n",
      "training epoch 1 : total loss: 1.3805895323175206\n",
      "training epoch 6 : total loss: 1.0816506432069768\n",
      "training epoch 11 : total loss: 0.8785415489662893\n",
      "training epoch 16 : total loss: 0.7151597301545806\n",
      "training epoch 21 : total loss: 0.6010107844281287\n",
      "training epoch 26 : total loss: 0.5111328088223915\n",
      "training epoch 31 : total loss: 0.4360248653605026\n",
      "training epoch 36 : total loss: 0.37479071648066736\n",
      "generating model with 6 parameters\n",
      "training start: initial total loss: 1.4337275481456881\n",
      "-----------------------------training begin--------------------------------\n",
      "training epoch 1 : total loss: 1.3592420614281477\n",
      "training epoch 6 : total loss: 1.064179823682023\n",
      "training epoch 11 : total loss: 0.8565469021957901\n",
      "training epoch 16 : total loss: 0.7031612009514516\n",
      "generating model with 6 parameters\n",
      "training start: initial total loss: 1.4111631220295493\n",
      "-----------------------------training begin--------------------------------\n",
      "training epoch 1 : total loss: 1.3390986229894393\n",
      "training epoch 6 : total loss: 1.0531921566479383\n",
      "training epoch 11 : total loss: 0.8515738935130432\n",
      "training epoch 16 : total loss: 0.7022931754774514\n",
      "training epoch 21 : total loss: 0.5878389703351371\n",
      "training epoch 26 : total loss: 0.4977352992988122\n",
      "training epoch 31 : total loss: 0.42529983058010545\n",
      "training epoch 36 : total loss: 0.36606804309108026\n",
      "generating model with 6 parameters\n",
      "training start: initial total loss: 1.4293962748167701\n",
      "-----------------------------training begin--------------------------------\n",
      "training epoch 1 : total loss: 1.2734928508349892\n",
      "training epoch 6 : total loss: 0.8103343456007743\n",
      "training epoch 11 : total loss: 0.553143499690792\n",
      "training epoch 16 : total loss: 0.40043376268558023\n",
      "generating model with 6 parameters\n",
      "training start: initial total loss: 1.395387266337302\n",
      "-----------------------------training begin--------------------------------\n",
      "training epoch 1 : total loss: 1.27208921168585\n",
      "training epoch 6 : total loss: 0.8129744407218971\n",
      "training epoch 11 : total loss: 0.5658132089265614\n",
      "training epoch 16 : total loss: 0.40925735785105793\n",
      "training epoch 21 : total loss: 0.3045922965485669\n",
      "training epoch 26 : total loss: 0.2351241156120515\n",
      "training epoch 31 : total loss: 0.17924702459128147\n",
      "training epoch 36 : total loss: 0.13862346448006804\n",
      "generating model with 6 parameters\n",
      "training start: initial total loss: 1.432029202639412\n",
      "-----------------------------training begin--------------------------------\n",
      "training epoch 1 : total loss: 1.2871261566506402\n",
      "training epoch 6 : total loss: 0.8204473756366284\n",
      "training epoch 11 : total loss: 0.5683214017868015\n",
      "training epoch 16 : total loss: 0.4124380417436342\n",
      "generating model with 6 parameters\n",
      "training start: initial total loss: 1.4485194850387422\n",
      "-----------------------------training begin--------------------------------\n",
      "training epoch 1 : total loss: 1.3042589050050046\n",
      "training epoch 6 : total loss: 0.833878297686374\n",
      "training epoch 11 : total loss: 0.5773300482908635\n",
      "training epoch 16 : total loss: 0.41816843426134387\n",
      "training epoch 21 : total loss: 0.31151474346393515\n",
      "training epoch 26 : total loss: 0.23639863524147503\n",
      "training epoch 31 : total loss: 0.18173475323596366\n",
      "training epoch 36 : total loss: 0.14111607577156313\n",
      "generating model with 6 parameters\n",
      "training start: initial total loss: 1.4169678333008286\n",
      "-----------------------------training begin--------------------------------\n",
      "training epoch 1 : total loss: 1.3507998996203072\n",
      "training epoch 6 : total loss: 1.0599927343241073\n",
      "training epoch 11 : total loss: 0.8558248081754168\n",
      "training epoch 16 : total loss: 0.7101557292770048\n",
      "generating model with 6 parameters\n",
      "training start: initial total loss: 1.4460867726170452\n",
      "-----------------------------training begin--------------------------------\n",
      "training epoch 1 : total loss: 1.368393393791946\n",
      "training epoch 6 : total loss: 1.06717461253516\n",
      "training epoch 11 : total loss: 0.8656401350170736\n",
      "training epoch 16 : total loss: 0.7100488981956029\n",
      "training epoch 21 : total loss: 0.5948601713351508\n",
      "training epoch 26 : total loss: 0.5037263028530969\n",
      "training epoch 31 : total loss: 0.43183649302615645\n",
      "training epoch 36 : total loss: 0.36890820120322315\n",
      "generating model with 6 parameters\n",
      "training start: initial total loss: 1.41906831289898\n",
      "-----------------------------training begin--------------------------------\n",
      "training epoch 1 : total loss: 1.3464804906719854\n",
      "training epoch 6 : total loss: 1.0589657904001126\n",
      "training epoch 11 : total loss: 0.8569251526175101\n",
      "training epoch 16 : total loss: 0.7078170634689971\n",
      "generating model with 6 parameters\n",
      "training start: initial total loss: 1.4195664857320933\n",
      "-----------------------------training begin--------------------------------\n",
      "training epoch 1 : total loss: 1.3476053108000436\n",
      "training epoch 6 : total loss: 1.0616244122303942\n",
      "training epoch 11 : total loss: 0.8594690592045678\n",
      "training epoch 16 : total loss: 0.7096176825625129\n",
      "training epoch 21 : total loss: 0.5946781523407322\n",
      "training epoch 26 : total loss: 0.5041574208619866\n",
      "training epoch 31 : total loss: 0.43137062540581805\n",
      "training epoch 36 : total loss: 0.3718404136335696\n",
      "generating model with 6 parameters\n",
      "training start: initial total loss: 1.453124769744498\n",
      "-----------------------------training begin--------------------------------\n",
      "training epoch 1 : total loss: 1.3080250837078613\n",
      "training epoch 6 : total loss: 0.8317915277115515\n",
      "training epoch 11 : total loss: 0.5730935753891554\n",
      "training epoch 16 : total loss: 0.41676042686034487\n",
      "generating model with 6 parameters\n",
      "training start: initial total loss: 1.4348404475076428\n",
      "-----------------------------training begin--------------------------------\n",
      "training epoch 1 : total loss: 1.2997725045407102\n",
      "training epoch 6 : total loss: 0.830435485712913\n",
      "training epoch 11 : total loss: 0.5730277199768373\n",
      "training epoch 16 : total loss: 0.41802640863451745\n",
      "training epoch 21 : total loss: 0.31007093434006483\n",
      "training epoch 26 : total loss: 0.23582123798292606\n",
      "training epoch 31 : total loss: 0.18214174569385166\n",
      "training epoch 36 : total loss: 0.14194012566008893\n",
      "generating model with 6 parameters\n",
      "training start: initial total loss: 1.4212476971635257\n",
      "-----------------------------training begin--------------------------------\n",
      "training epoch 1 : total loss: 1.2784542730417436\n",
      "training epoch 6 : total loss: 0.8168386120781753\n",
      "training epoch 11 : total loss: 0.567709627919762\n",
      "training epoch 16 : total loss: 0.4141503578657327\n",
      "generating model with 6 parameters\n",
      "training start: initial total loss: 1.4296311818380343\n",
      "-----------------------------training begin--------------------------------\n",
      "training epoch 1 : total loss: 1.286910139842699\n",
      "training epoch 6 : total loss: 0.8201023974705598\n",
      "training epoch 11 : total loss: 0.5659928095343483\n",
      "training epoch 16 : total loss: 0.40947546890947595\n",
      "training epoch 21 : total loss: 0.30547998544591864\n",
      "training epoch 26 : total loss: 0.2327376523768\n",
      "training epoch 31 : total loss: 0.18004701154116237\n",
      "training epoch 36 : total loss: 0.14095288036427572\n",
      "generating model with 6 parameters\n",
      "training start: initial total loss: 1.4502843692634322\n",
      "-----------------------------training begin--------------------------------\n",
      "training epoch 1 : total loss: 1.3747919871905947\n",
      "training epoch 6 : total loss: 1.0851088289923925\n",
      "training epoch 11 : total loss: 0.8713544238260137\n",
      "training epoch 16 : total loss: 0.7179774324698782\n",
      "generating model with 6 parameters\n",
      "training start: initial total loss: 1.4215623823143368\n",
      "-----------------------------training begin--------------------------------\n",
      "training epoch 1 : total loss: 1.3502127970057265\n",
      "training epoch 6 : total loss: 1.0566820270998567\n",
      "training epoch 11 : total loss: 0.8551772099350615\n",
      "training epoch 16 : total loss: 0.7096039635693489\n",
      "training epoch 21 : total loss: 0.592859014957442\n",
      "training epoch 26 : total loss: 0.5068749368212935\n",
      "training epoch 31 : total loss: 0.4351941031351239\n",
      "training epoch 36 : total loss: 0.37818494490351445\n",
      "generating model with 6 parameters\n",
      "training start: initial total loss: 1.4473479287427091\n",
      "-----------------------------training begin--------------------------------\n",
      "training epoch 1 : total loss: 1.3701330052542464\n",
      "training epoch 6 : total loss: 1.0711200714968871\n",
      "training epoch 11 : total loss: 0.8674344632426062\n",
      "training epoch 16 : total loss: 0.7207984837860284\n",
      "generating model with 6 parameters\n",
      "training start: initial total loss: 1.4297798643131732\n",
      "-----------------------------training begin--------------------------------\n",
      "training epoch 1 : total loss: 1.354736657583083\n",
      "training epoch 6 : total loss: 1.0618402847166841\n",
      "training epoch 11 : total loss: 0.8608027466976061\n",
      "training epoch 16 : total loss: 0.7155434839214703\n",
      "training epoch 21 : total loss: 0.6066056860723308\n",
      "training epoch 26 : total loss: 0.5225297272154494\n",
      "training epoch 31 : total loss: 0.45611293209451553\n",
      "training epoch 36 : total loss: 0.40262110233479376\n",
      "generating model with 6 parameters\n",
      "training start: initial total loss: 1.4342906023197153\n",
      "-----------------------------training begin--------------------------------\n",
      "training epoch 1 : total loss: 1.2790054128575121\n",
      "training epoch 6 : total loss: 0.8185153749052235\n",
      "training epoch 11 : total loss: 0.5759367970669217\n",
      "training epoch 16 : total loss: 0.4231182146775866\n",
      "generating model with 6 parameters\n",
      "training start: initial total loss: 1.4458303071106762\n",
      "-----------------------------training begin--------------------------------\n",
      "training epoch 1 : total loss: 1.2985160437687784\n",
      "training epoch 6 : total loss: 0.8277305605949344\n",
      "training epoch 11 : total loss: 0.5784272772950745\n",
      "training epoch 16 : total loss: 0.43135459099032614\n",
      "training epoch 21 : total loss: 0.32681951812415466\n",
      "training epoch 26 : total loss: 0.25709594002855535\n",
      "training epoch 31 : total loss: 0.20608748311509953\n",
      "training epoch 36 : total loss: 0.16742769283311576\n",
      "generating model with 6 parameters\n",
      "training start: initial total loss: 1.4257971894408823\n",
      "-----------------------------training begin--------------------------------\n",
      "training epoch 1 : total loss: 1.2784906117028925\n",
      "training epoch 6 : total loss: 0.8156177837082668\n",
      "training epoch 11 : total loss: 0.5758259211893423\n",
      "training epoch 16 : total loss: 0.43300560288295137\n",
      "generating model with 6 parameters\n",
      "training start: initial total loss: 1.434375624893824\n",
      "-----------------------------training begin--------------------------------\n",
      "training epoch 1 : total loss: 1.2880257266363726\n",
      "training epoch 6 : total loss: 0.8265609245311936\n",
      "training epoch 11 : total loss: 0.5864050124765964\n",
      "training epoch 16 : total loss: 0.4428432368525256\n",
      "training epoch 21 : total loss: 0.3493530344893822\n",
      "training epoch 26 : total loss: 0.2846824745940074\n",
      "training epoch 31 : total loss: 0.23793209500093473\n",
      "training epoch 36 : total loss: 0.2029522259882396\n",
      "generating model with 6 parameters\n",
      "training start: initial total loss: 1.4035257106485475\n",
      "-----------------------------training begin--------------------------------\n",
      "training epoch 1 : total loss: 1.27850871258733\n",
      "training epoch 6 : total loss: 1.029164177560348\n",
      "training epoch 11 : total loss: 0.9167716538066297\n",
      "training epoch 16 : total loss: 0.830841673764541\n",
      "generating model with 6 parameters\n",
      "training start: initial total loss: 1.422205867811642\n",
      "-----------------------------training begin--------------------------------\n",
      "training epoch 1 : total loss: 1.2776498730818928\n",
      "training epoch 6 : total loss: 1.0366504214528451\n",
      "training epoch 11 : total loss: 0.9204022441137285\n",
      "training epoch 16 : total loss: 0.8314553288097246\n",
      "training epoch 21 : total loss: 0.7689802196902188\n",
      "training epoch 26 : total loss: 0.706336201439739\n",
      "training epoch 31 : total loss: 0.6546371852213551\n",
      "training epoch 36 : total loss: 0.6171513070011211\n",
      "generating model with 6 parameters\n",
      "training start: initial total loss: 1.4350877385594032\n",
      "-----------------------------training begin--------------------------------\n",
      "training epoch 1 : total loss: 1.248247780375297\n",
      "training epoch 6 : total loss: 1.05297650946659\n",
      "training epoch 11 : total loss: 0.937869693887866\n",
      "training epoch 16 : total loss: 0.8464476122279521\n",
      "generating model with 6 parameters\n",
      "training start: initial total loss: 1.4510795301162194\n",
      "-----------------------------training begin--------------------------------\n",
      "training epoch 1 : total loss: 1.2576190663743496\n",
      "training epoch 6 : total loss: 1.054385014665671\n",
      "training epoch 11 : total loss: 0.9381255369567838\n",
      "training epoch 16 : total loss: 0.8464705343647481\n",
      "training epoch 21 : total loss: 0.772325125607012\n",
      "training epoch 26 : total loss: 0.7112628546561708\n",
      "training epoch 31 : total loss: 0.6601940903895236\n",
      "training epoch 36 : total loss: 0.6168835368227847\n",
      "generating model with 6 parameters\n",
      "training start: initial total loss: 1.4418008660870894\n",
      "-----------------------------training begin--------------------------------\n",
      "training epoch 1 : total loss: 1.2098087431371576\n",
      "training epoch 6 : total loss: 0.8999282515890987\n",
      "training epoch 11 : total loss: 0.7537074752984799\n",
      "training epoch 16 : total loss: 0.6470525374977629\n",
      "generating model with 6 parameters\n",
      "training start: initial total loss: 1.4264637255906871\n",
      "-----------------------------training begin--------------------------------\n",
      "training epoch 1 : total loss: 1.2056095667122648\n",
      "training epoch 6 : total loss: 0.9031551788910277\n",
      "training epoch 11 : total loss: 0.751152946930016\n",
      "training epoch 16 : total loss: 0.6478369737405794\n",
      "training epoch 21 : total loss: 0.5719600094852567\n",
      "training epoch 26 : total loss: 0.5181443611037598\n",
      "training epoch 31 : total loss: 0.4674102006902683\n",
      "training epoch 36 : total loss: 0.42719582139945106\n",
      "generating model with 6 parameters\n",
      "training start: initial total loss: 1.4363814809764284\n",
      "-----------------------------training begin--------------------------------\n",
      "training epoch 1 : total loss: 1.1816105472882559\n",
      "training epoch 6 : total loss: 0.916500544628412\n",
      "training epoch 11 : total loss: 0.7573359999267603\n",
      "training epoch 16 : total loss: 0.6491996773545392\n",
      "generating model with 6 parameters\n",
      "training start: initial total loss: 1.4309493466380174\n",
      "-----------------------------training begin--------------------------------\n",
      "training epoch 1 : total loss: 1.1809010585020414\n",
      "training epoch 6 : total loss: 0.9166252990186549\n",
      "training epoch 11 : total loss: 0.7573968749675313\n",
      "training epoch 16 : total loss: 0.6492324056401586\n",
      "training epoch 21 : total loss: 0.5712681680247603\n",
      "training epoch 26 : total loss: 0.5124217229168361\n",
      "training epoch 31 : total loss: 0.4663753207057214\n",
      "training epoch 36 : total loss: 0.4293094768581709\n"
     ]
    }
   ],
   "source": [
    "cal={}\n",
    "train_error_={}\n",
    "for i in lambda_list:\n",
    "    for j in learning_rate_list:\n",
    "        for k in batch_size:\n",
    "            for p in epochs:\n",
    "                model=learning(try_data,k=6,lambda_=i,learning_rate=j,B=k,epoch=p,show_epoch=5)\n",
    "                model.fit(train_temp_data)\n",
    "                cal[(i,j,k,p)]=model.all_loss(validation_temp_data)\n",
    "                train_error_[(i,j,k,p)]=model.all_loss(train_temp_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "li_top=[]\n",
    "for i in cal:\n",
    "    li_top.append((i,cal[i]))\n",
    "li_top.sort(key=lambda x: x[1])\n",
    "\n",
    "\n",
    "show_dict={}\n",
    "show_dict[\"lambda\"]=[]\n",
    "show_dict[\"learning_rate\"]=[]\n",
    "show_dict[\"best_batch\"]=[]\n",
    "show_dict[\"best_epoch\"]=[]\n",
    "show_dict[\"validate_error\"]=[]\n",
    "show_dict[\"train_error\"]=[]\n",
    "\n",
    "\n",
    "\n",
    "for i in li_top:\n",
    "    show_dict[\"lambda\"].append(i[0][0])\n",
    "    show_dict[\"learning_rate\"].append(i[0][1])\n",
    "    show_dict[\"best_batch\"].append(i[0][2])\n",
    "    show_dict[\"best_epoch\"].append(i[0][3])\n",
    "    show_dict[\"train_error\"].append(train_error_[i[0]])\n",
    "    show_dict[\"validate_error\"].append(i[1])\n",
    "\n",
    "import pandas as pd\n",
    "data_frame=pd.DataFrame(show_dict)\n",
    "data_frame.to_csv(\"training_log_one_101.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lambda</th>\n",
       "      <th>learning_rate</th>\n",
       "      <th>best_batch</th>\n",
       "      <th>best_epoch</th>\n",
       "      <th>validate_error</th>\n",
       "      <th>train_error</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>100.00</td>\n",
       "      <td>0.010</td>\n",
       "      <td>400</td>\n",
       "      <td>20</td>\n",
       "      <td>1.126210</td>\n",
       "      <td>0.583202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>100.00</td>\n",
       "      <td>0.005</td>\n",
       "      <td>400</td>\n",
       "      <td>20</td>\n",
       "      <td>1.131304</td>\n",
       "      <td>0.779223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>100.00</td>\n",
       "      <td>0.005</td>\n",
       "      <td>1000</td>\n",
       "      <td>20</td>\n",
       "      <td>1.136494</td>\n",
       "      <td>0.786024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>100.00</td>\n",
       "      <td>0.010</td>\n",
       "      <td>1000</td>\n",
       "      <td>20</td>\n",
       "      <td>1.138470</td>\n",
       "      <td>0.585058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>100.00</td>\n",
       "      <td>0.005</td>\n",
       "      <td>1000</td>\n",
       "      <td>40</td>\n",
       "      <td>1.143802</td>\n",
       "      <td>0.586706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>100.00</td>\n",
       "      <td>0.010</td>\n",
       "      <td>400</td>\n",
       "      <td>40</td>\n",
       "      <td>1.145284</td>\n",
       "      <td>0.404207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>100.00</td>\n",
       "      <td>0.005</td>\n",
       "      <td>400</td>\n",
       "      <td>40</td>\n",
       "      <td>1.151217</td>\n",
       "      <td>0.589734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>100.00</td>\n",
       "      <td>0.010</td>\n",
       "      <td>1000</td>\n",
       "      <td>40</td>\n",
       "      <td>1.188150</td>\n",
       "      <td>0.404456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.005</td>\n",
       "      <td>1000</td>\n",
       "      <td>20</td>\n",
       "      <td>1.217964</td>\n",
       "      <td>0.630661</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.005</td>\n",
       "      <td>1000</td>\n",
       "      <td>40</td>\n",
       "      <td>1.239300</td>\n",
       "      <td>0.366927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.010</td>\n",
       "      <td>1000</td>\n",
       "      <td>20</td>\n",
       "      <td>1.240711</td>\n",
       "      <td>0.356063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.010</td>\n",
       "      <td>400</td>\n",
       "      <td>20</td>\n",
       "      <td>1.266176</td>\n",
       "      <td>0.342125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.010</td>\n",
       "      <td>400</td>\n",
       "      <td>20</td>\n",
       "      <td>1.267958</td>\n",
       "      <td>0.316329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.005</td>\n",
       "      <td>1000</td>\n",
       "      <td>40</td>\n",
       "      <td>1.272027</td>\n",
       "      <td>0.326075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.005</td>\n",
       "      <td>400</td>\n",
       "      <td>20</td>\n",
       "      <td>1.280143</td>\n",
       "      <td>0.611989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.005</td>\n",
       "      <td>1000</td>\n",
       "      <td>20</td>\n",
       "      <td>1.282163</td>\n",
       "      <td>0.614343</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.010</td>\n",
       "      <td>1000</td>\n",
       "      <td>40</td>\n",
       "      <td>1.284437</td>\n",
       "      <td>0.180940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.005</td>\n",
       "      <td>400</td>\n",
       "      <td>40</td>\n",
       "      <td>1.289423</td>\n",
       "      <td>0.338835</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.005</td>\n",
       "      <td>400</td>\n",
       "      <td>20</td>\n",
       "      <td>1.289610</td>\n",
       "      <td>0.629619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.005</td>\n",
       "      <td>400</td>\n",
       "      <td>40</td>\n",
       "      <td>1.303651</td>\n",
       "      <td>0.329322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.010</td>\n",
       "      <td>1000</td>\n",
       "      <td>20</td>\n",
       "      <td>1.306723</td>\n",
       "      <td>0.329144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.010</td>\n",
       "      <td>1000</td>\n",
       "      <td>40</td>\n",
       "      <td>1.309783</td>\n",
       "      <td>0.116726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.010</td>\n",
       "      <td>400</td>\n",
       "      <td>20</td>\n",
       "      <td>1.312849</td>\n",
       "      <td>0.328541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.005</td>\n",
       "      <td>1000</td>\n",
       "      <td>20</td>\n",
       "      <td>1.316624</td>\n",
       "      <td>0.607029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.005</td>\n",
       "      <td>1000</td>\n",
       "      <td>40</td>\n",
       "      <td>1.330411</td>\n",
       "      <td>0.331642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.010</td>\n",
       "      <td>1000</td>\n",
       "      <td>20</td>\n",
       "      <td>1.330831</td>\n",
       "      <td>0.325999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.005</td>\n",
       "      <td>400</td>\n",
       "      <td>40</td>\n",
       "      <td>1.331800</td>\n",
       "      <td>0.335064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.005</td>\n",
       "      <td>400</td>\n",
       "      <td>20</td>\n",
       "      <td>1.335374</td>\n",
       "      <td>0.621762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.010</td>\n",
       "      <td>400</td>\n",
       "      <td>40</td>\n",
       "      <td>1.340779</td>\n",
       "      <td>0.117772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.010</td>\n",
       "      <td>400</td>\n",
       "      <td>40</td>\n",
       "      <td>1.354626</td>\n",
       "      <td>0.142068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.010</td>\n",
       "      <td>400</td>\n",
       "      <td>40</td>\n",
       "      <td>1.361439</td>\n",
       "      <td>0.115007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.010</td>\n",
       "      <td>1000</td>\n",
       "      <td>40</td>\n",
       "      <td>1.362425</td>\n",
       "      <td>0.115967</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    lambda  learning_rate  best_batch  best_epoch  validate_error  train_error\n",
       "0   100.00          0.010         400          20        1.126210     0.583202\n",
       "1   100.00          0.005         400          20        1.131304     0.779223\n",
       "2   100.00          0.005        1000          20        1.136494     0.786024\n",
       "3   100.00          0.010        1000          20        1.138470     0.585058\n",
       "4   100.00          0.005        1000          40        1.143802     0.586706\n",
       "5   100.00          0.010         400          40        1.145284     0.404207\n",
       "6   100.00          0.005         400          40        1.151217     0.589734\n",
       "7   100.00          0.010        1000          40        1.188150     0.404456\n",
       "8     1.00          0.005        1000          20        1.217964     0.630661\n",
       "9     1.00          0.005        1000          40        1.239300     0.366927\n",
       "10    1.00          0.010        1000          20        1.240711     0.356063\n",
       "11    1.00          0.010         400          20        1.266176     0.342125\n",
       "12    0.01          0.010         400          20        1.267958     0.316329\n",
       "13    0.01          0.005        1000          40        1.272027     0.326075\n",
       "14    0.10          0.005         400          20        1.280143     0.611989\n",
       "15    0.10          0.005        1000          20        1.282163     0.614343\n",
       "16    1.00          0.010        1000          40        1.284437     0.180940\n",
       "17    1.00          0.005         400          40        1.289423     0.338835\n",
       "18    1.00          0.005         400          20        1.289610     0.629619\n",
       "19    0.10          0.005         400          40        1.303651     0.329322\n",
       "20    0.10          0.010        1000          20        1.306723     0.329144\n",
       "21    0.10          0.010        1000          40        1.309783     0.116726\n",
       "22    0.10          0.010         400          20        1.312849     0.328541\n",
       "23    0.01          0.005        1000          20        1.316624     0.607029\n",
       "24    0.10          0.005        1000          40        1.330411     0.331642\n",
       "25    0.01          0.010        1000          20        1.330831     0.325999\n",
       "26    0.01          0.005         400          40        1.331800     0.335064\n",
       "27    0.01          0.005         400          20        1.335374     0.621762\n",
       "28    0.10          0.010         400          40        1.340779     0.117772\n",
       "29    1.00          0.010         400          40        1.354626     0.142068\n",
       "30    0.01          0.010         400          40        1.361439     0.115007\n",
       "31    0.01          0.010        1000          40        1.362425     0.115967"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_frame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best parameters for F=6 as followed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((100, 0.01, 400, 20), 1.1262104269605353)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "li_top[0]\n",
    "#  lambda, learning rate, best batch, best epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_lambda,best_learning_rate,best_batch,best_epoch=li_top[0][0][0],li_top[0][0][1],li_top[0][0][2],li_top[0][0][3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## using the validation data to select"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_here,validate_here=train_test_split(train,test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we will roughtly select a F here\n",
    "\n",
    "using the validation set's error as metric to select the best 3 Fs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "K=[i for i in range(0,11)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generating model with 0 parameters\n",
      "training start: initial total loss: 1.2486645657192947\n",
      "-----------------------------training begin--------------------------------\n",
      "training epoch 1 : total loss: 0.8465046532431485\n",
      "training epoch 6 : total loss: 0.8079310315598908\n",
      "training epoch 11 : total loss: 0.8054269638906147\n",
      "training epoch 16 : total loss: 0.8049666655236793\n",
      "generating model with 1 parameters\n",
      "training start: initial total loss: 2.2621074230203586\n",
      "-----------------------------training begin--------------------------------\n",
      "training epoch 1 : total loss: 0.8700371514490307\n",
      "training epoch 6 : total loss: 0.8044429252919171\n",
      "training epoch 11 : total loss: 0.79808015124735\n",
      "training epoch 16 : total loss: 0.7874607600266397\n",
      "generating model with 2 parameters\n",
      "training start: initial total loss: 1.7427618183137858\n",
      "-----------------------------training begin--------------------------------\n",
      "training epoch 1 : total loss: 0.8681624811210247\n",
      "training epoch 6 : total loss: 0.8015402598316885\n",
      "training epoch 11 : total loss: 0.7950758347435809\n",
      "training epoch 16 : total loss: 0.7812379131279896\n",
      "generating model with 3 parameters\n",
      "training start: initial total loss: 1.5846121179821697\n",
      "-----------------------------training begin--------------------------------\n",
      "training epoch 1 : total loss: 0.865916772231866\n",
      "training epoch 6 : total loss: 0.7973795440176389\n",
      "training epoch 11 : total loss: 0.7831272337408982\n",
      "training epoch 16 : total loss: 0.7571393040631561\n",
      "generating model with 4 parameters\n",
      "training start: initial total loss: 1.5126622193786274\n",
      "-----------------------------training begin--------------------------------\n",
      "training epoch 1 : total loss: 0.8632054791261726\n",
      "training epoch 6 : total loss: 0.7952897625451603\n",
      "training epoch 11 : total loss: 0.784206392191566\n",
      "training epoch 16 : total loss: 0.7574843780475743\n",
      "generating model with 5 parameters\n",
      "training start: initial total loss: 1.4463416899646733\n",
      "-----------------------------training begin--------------------------------\n",
      "training epoch 1 : total loss: 0.861081879829979\n",
      "training epoch 6 : total loss: 0.7918635696846682\n",
      "training epoch 11 : total loss: 0.7667280843562916\n",
      "training epoch 16 : total loss: 0.737477567387631\n",
      "generating model with 6 parameters\n",
      "training start: initial total loss: 1.4195246737022738\n",
      "-----------------------------training begin--------------------------------\n",
      "training epoch 1 : total loss: 0.8591488148240305\n",
      "training epoch 6 : total loss: 0.7890828523044056\n",
      "training epoch 11 : total loss: 0.7619706974141486\n",
      "training epoch 16 : total loss: 0.7317301071530685\n",
      "generating model with 7 parameters\n",
      "training start: initial total loss: 1.3910356650942637\n",
      "-----------------------------training begin--------------------------------\n",
      "training epoch 1 : total loss: 0.8569163147832968\n",
      "training epoch 6 : total loss: 0.7886738453699871\n",
      "training epoch 11 : total loss: 0.7646240320280634\n",
      "training epoch 16 : total loss: 0.7280692843013745\n",
      "generating model with 8 parameters\n",
      "training start: initial total loss: 1.3729408871404436\n",
      "-----------------------------training begin--------------------------------\n",
      "training epoch 1 : total loss: 0.855278249851007\n",
      "training epoch 6 : total loss: 0.7852963583489171\n",
      "training epoch 11 : total loss: 0.7511702932740796\n",
      "training epoch 16 : total loss: 0.7172870548789577\n",
      "generating model with 9 parameters\n",
      "training start: initial total loss: 1.3590905455589941\n",
      "-----------------------------training begin--------------------------------\n",
      "training epoch 1 : total loss: 0.8539699367320738\n",
      "training epoch 6 : total loss: 0.7835391148513211\n",
      "training epoch 11 : total loss: 0.7484153793501562\n",
      "training epoch 16 : total loss: 0.7125717140675766\n",
      "generating model with 10 parameters\n",
      "training start: initial total loss: 1.3532697483891354\n",
      "-----------------------------training begin--------------------------------\n",
      "training epoch 1 : total loss: 0.8526398508794428\n",
      "training epoch 6 : total loss: 0.7801757069706337\n",
      "training epoch 11 : total loss: 0.7407325629536734\n",
      "training epoch 16 : total loss: 0.7032384413147682\n"
     ]
    }
   ],
   "source": [
    "store=[]\n",
    "train_err=[]\n",
    "for F in K:\n",
    "    model=learning(try_data,k=F,lambda_=best_lambda,learning_rate=best_learning_rate,B=best_batch,epoch=best_epoch,show_epoch=5)\n",
    "    model.fit(train_here)\n",
    "    store.append((F,model.all_loss(validate_here)))\n",
    "    train_err.append(model.all_loss(train_here))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_dict1={}\n",
    "show_dict1[\"F\"]=[]\n",
    "show_dict1['train_error']=[]\n",
    "show_dict1['validate_error']=[]\n",
    "\n",
    "for i in range(len(store)):\n",
    "    show_dict1[\"F\"].append(K[i])\n",
    "    show_dict1[\"train_error\"].append(train_err[i])\n",
    "    show_dict1[\"validate_error\"].append(store[i][1])\n",
    "\n",
    "data_frame2=pd.DataFrame(show_dict1)\n",
    "data_frame2.to_csv(\"F_candidates.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets choose the best F candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best 3 F is: \n",
      "10\n",
      "9\n",
      "8\n"
     ]
    }
   ],
   "source": [
    "store.sort(key=lambda x: x[1])\n",
    "store=[i for i in store if i[0]!=0]\n",
    "print(\"Best 3 F is: \")\n",
    "for i in store[:3]:\n",
    "    if i[0]!=0:\n",
    "        print(i[0])\n",
    "best_F=[i[0] for i in store[:3]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### best 3 candidate with 5-Fold cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "kf=KFold(5,shuffle=True)\n",
    "folds=list(kf.split(train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(160034,)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "folds[1][1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_cross_validation(model,X,folds):\n",
    "    '''\n",
    "    a modified function for the testing\n",
    "    '''\n",
    "    kfolds=len(folds)\n",
    "    train_performance=np.empty(kfolds)\n",
    "    validation_performance=np.empty(kfolds)\n",
    "    for idx in range(kfolds):\n",
    "        train,validation=folds[idx]\n",
    "        X_train=X[train]\n",
    "        \n",
    "        model.fit(X_train)\n",
    "        \n",
    "        train_loss=model.all_loss(X_train)\n",
    "        \n",
    "        X_validation=X[validation]\n",
    "        \n",
    "        \n",
    "        validation_loss=model.all_loss(X_validation)\n",
    "        \n",
    "        train_performance[idx]=train_loss\n",
    "        validation_performance[idx]=validation_loss\n",
    "        \n",
    "    return np.array(train_performance),np.array(validation_performance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generating model with 10 parameters\n",
      "training start: initial total loss: 1.3470806863135412\n",
      "-----------------------------training begin--------------------------------\n",
      "training epoch 1 : total loss: 0.8511560585988706\n",
      "training epoch 6 : total loss: 0.7814850981020013\n",
      "training epoch 11 : total loss: 0.7449350096371293\n",
      "training epoch 16 : total loss: 0.7040576533836763\n",
      "training start: initial total loss: 1.352472765363259\n",
      "-----------------------------training begin--------------------------------\n",
      "training epoch 1 : total loss: 0.8522296403659213\n",
      "training epoch 6 : total loss: 0.7816795496363983\n",
      "training epoch 11 : total loss: 0.7416835803001238\n",
      "training epoch 16 : total loss: 0.7008345417065189\n",
      "training start: initial total loss: 1.3480157064147154\n",
      "-----------------------------training begin--------------------------------\n",
      "training epoch 1 : total loss: 0.8525384320977999\n",
      "training epoch 6 : total loss: 0.7824391563890388\n",
      "training epoch 11 : total loss: 0.7470818673476234\n",
      "training epoch 16 : total loss: 0.7071542119829313\n",
      "training start: initial total loss: 1.34547639827994\n",
      "-----------------------------training begin--------------------------------\n",
      "training epoch 1 : total loss: 0.8510881983347137\n",
      "training epoch 6 : total loss: 0.7793616226267374\n",
      "training epoch 11 : total loss: 0.739578153321071\n",
      "training epoch 16 : total loss: 0.7054099466709471\n",
      "training start: initial total loss: 1.3507916621158385\n",
      "-----------------------------training begin--------------------------------\n",
      "training epoch 1 : total loss: 0.8530088975509275\n",
      "training epoch 6 : total loss: 0.7823833376688016\n",
      "training epoch 11 : total loss: 0.7453142613165655\n",
      "training epoch 16 : total loss: 0.708260549146113\n",
      "generating model with 9 parameters\n",
      "training start: initial total loss: 1.3587753197672483\n",
      "-----------------------------training begin--------------------------------\n",
      "training epoch 1 : total loss: 0.8523557991575497\n",
      "training epoch 6 : total loss: 0.781547323779538\n",
      "training epoch 11 : total loss: 0.7462511564481098\n",
      "training epoch 16 : total loss: 0.7131236804093453\n",
      "training start: initial total loss: 1.3604212887082903\n",
      "-----------------------------training begin--------------------------------\n",
      "training epoch 1 : total loss: 0.8536850551852784\n",
      "training epoch 6 : total loss: 0.7840666244429283\n",
      "training epoch 11 : total loss: 0.7478995570404107\n",
      "training epoch 16 : total loss: 0.7106986487613541\n",
      "training start: initial total loss: 1.3574439416774988\n",
      "-----------------------------training begin--------------------------------\n",
      "training epoch 1 : total loss: 0.8536617922745747\n",
      "training epoch 6 : total loss: 0.7847084835770846\n",
      "training epoch 11 : total loss: 0.7502650338891589\n",
      "training epoch 16 : total loss: 0.7117973774389238\n",
      "training start: initial total loss: 1.3548052712340408\n",
      "-----------------------------training begin--------------------------------\n",
      "training epoch 1 : total loss: 0.8515732907169947\n",
      "training epoch 6 : total loss: 0.7810729600814482\n",
      "training epoch 11 : total loss: 0.744877846916975\n",
      "training epoch 16 : total loss: 0.7108371965965523\n",
      "training start: initial total loss: 1.356454581885563\n",
      "-----------------------------training begin--------------------------------\n",
      "training epoch 1 : total loss: 0.8534889315673193\n",
      "training epoch 6 : total loss: 0.7841781155026497\n",
      "training epoch 11 : total loss: 0.750664134133389\n",
      "training epoch 16 : total loss: 0.7131090088972272\n",
      "generating model with 8 parameters\n",
      "training start: initial total loss: 1.3700044080287106\n",
      "-----------------------------training begin--------------------------------\n",
      "training epoch 1 : total loss: 0.8532691935615571\n",
      "training epoch 6 : total loss: 0.784190490241734\n",
      "training epoch 11 : total loss: 0.7511658525769699\n",
      "training epoch 16 : total loss: 0.7142284409478198\n",
      "training start: initial total loss: 1.3801079701082515\n",
      "-----------------------------training begin--------------------------------\n",
      "training epoch 1 : total loss: 0.8557796613322889\n",
      "training epoch 6 : total loss: 0.784897739168657\n",
      "training epoch 11 : total loss: 0.748829326780423\n",
      "training epoch 16 : total loss: 0.7112505647792127\n",
      "training start: initial total loss: 1.3781356533500506\n",
      "-----------------------------training begin--------------------------------\n",
      "training epoch 1 : total loss: 0.8556692444277658\n",
      "training epoch 6 : total loss: 0.7863265236588289\n",
      "training epoch 11 : total loss: 0.7545280227611484\n",
      "training epoch 16 : total loss: 0.7201181894850402\n",
      "training start: initial total loss: 1.372519332732543\n",
      "-----------------------------training begin--------------------------------\n",
      "training epoch 1 : total loss: 0.8537044186196718\n",
      "training epoch 6 : total loss: 0.7842137384464899\n",
      "training epoch 11 : total loss: 0.7509818897385646\n",
      "training epoch 16 : total loss: 0.7155129967076015\n",
      "training start: initial total loss: 1.3739153044215944\n",
      "-----------------------------training begin--------------------------------\n",
      "training epoch 1 : total loss: 0.8554761019446093\n",
      "training epoch 6 : total loss: 0.7861743491182808\n",
      "training epoch 11 : total loss: 0.7542486984675347\n",
      "training epoch 16 : total loss: 0.7180085270295435\n"
     ]
    }
   ],
   "source": [
    "train_list={}\n",
    "val_list={}\n",
    "for i in best_F:\n",
    "    model=learning(try_data,k=i,lambda_=best_lambda,learning_rate=best_learning_rate,B=best_batch,epoch=best_epoch,show_epoch=5)\n",
    "    train_error_array,validate_error_array=model_cross_validation(model,train,folds)\n",
    "    train_list[i]=train_error_array.mean()\n",
    "    val_list[i]=validate_error_array.mean()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## the result of our 5-Fold model selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>validate_error</th>\n",
       "      <th>train_error</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.776678</td>\n",
       "      <td>0.681839</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.780665</td>\n",
       "      <td>0.689586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.780369</td>\n",
       "      <td>0.693600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    validate_error  train_error\n",
       "10        0.776678     0.681839\n",
       "9         0.780665     0.689586\n",
       "8         0.780369     0.693600"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a=pd.DataFrame(pd.Series(data=val_list),columns=['validate_error'])\n",
    "a.join(pd.DataFrame(pd.Series(data=train_list),columns=['train_error']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "F_sort=[(i,val_list[i]) for i in val_list]\n",
    "best_F=sorted(F_sort,key=lambda x:x[1])[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# now let's test on the test dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Best F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the 5-kfold's result, the best F should be 10. Lets compare it with F=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generating model with 10 parameters\n",
      "training start: initial total loss: 1.3479610627117045\n",
      "-----------------------------training begin--------------------------------\n",
      "training epoch 1 : total loss: 0.8441231612011936\n",
      "training epoch 6 : total loss: 0.781728260042608\n",
      "training epoch 11 : total loss: 0.730213679105499\n",
      "training epoch 16 : total loss: 0.6897087078291758\n",
      "Model with F=10 has MSE as:0.7614268992800404\n"
     ]
    }
   ],
   "source": [
    "model=learning(try_data,k=best_F,lambda_=best_lambda,learning_rate=best_learning_rate,B=best_batch,epoch=best_epoch,show_epoch=5)\n",
    "model.fit(train)\n",
    "error=model.all_loss(test)\n",
    "print(\"Model with F={} has MSE as:{}\".format(best_F,error))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "F=10, error on test dataset is 0.7614"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## F=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generating model with 0 parameters\n",
      "training start: initial total loss: 1.2480347253092772\n",
      "-----------------------------training begin--------------------------------\n",
      "training epoch 1 : total loss: 0.8397271808388129\n",
      "training epoch 6 : total loss: 0.8091721252700773\n",
      "training epoch 11 : total loss: 0.8081036399713465\n",
      "training epoch 16 : total loss: 0.8079117030913532\n",
      "Model with F=0 has MSE as:0.8287313445550961\n"
     ]
    }
   ],
   "source": [
    "model=learning(try_data,k=0,lambda_=best_lambda,learning_rate=best_learning_rate,B=best_batch,epoch=best_epoch,show_epoch=5)\n",
    "model.fit(train)\n",
    "error=model.all_loss(test)\n",
    "print(\"Model with F={} has MSE as:{}\".format(0,error))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "F=0, error on test dataset is 0.8287"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the test dataset, we can see that when the embedding help develop the accuracy of our recommendation system. The test error on F=10 is 0.7614 while the MSE in F=0 of test dataset is 0.8287, Thus, we can say the Embedding model performs better than F=0 the popularity model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If given more time on finding the best hyper-parameter in a larger training dataset, probably we can find a better recommendation parameters and the corresponding hyper-parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
